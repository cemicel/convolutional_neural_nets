{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all needed librariesfor c, i in enumerate(paths):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy import misc\n",
    "import scipy.misc\n",
    "import operator\n",
    "import functools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for generating data both training and testing.\n",
    "# Data should be stored in separate files with names i.png (0<=i<=n)\n",
    "# Parameters: \n",
    "# - num_of_pics - provide particular number of pictures to be extracted,\n",
    "#  if None extracts all.\n",
    "# - for_test - number of pics for testing. Cannot be bigger than 30% or less that 1\n",
    "#  by default 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/data_text_form'\n",
    "\n",
    "def get_data(path_r, num_of_pics=None, for_test=None):\n",
    "    paths = [x[0] for c, x in enumerate(os.walk(os.getcwd()+path_r)) if c > 0]\n",
    "    f = paths[1]\n",
    "    ath, dirs, files = next(os.walk(f))\n",
    "    file_count = len(files) - 1\n",
    "    cou = 1\n",
    "    test_input = []\n",
    "    test_label = []\n",
    "    train_input = []\n",
    "    train_label = []\n",
    "    if num_of_pics is None:\n",
    "        num_of_pics = file_count\n",
    "    if for_test is None and num_of_pics is None:\n",
    "        for_test = file_count * len(paths) * .01\n",
    "    elif for_test is None and num_of_pics is not None:\n",
    "        for_test = num_of_pics * len(paths) * .01\n",
    "    elif for_test < num_of_pics * len(paths) * .01:\n",
    "        raise ValueError('Not enough for testing')\n",
    "    elif for_test > file_count * len(paths) * .3:\n",
    "        raise ValueError('Too many for testing')\n",
    "\n",
    "    for p_count, path in enumerate(paths):\n",
    "        one_hot_enc_arr = np.zeros(len(paths))\n",
    "        for pic in range(num_of_pics):\n",
    "            one_hot_enc_arr[p_count] = 1\n",
    "\n",
    "            if pic < for_test:\n",
    "                test_input.append(scipy.misc.imread(path + '/{}.png'.format(pic), mode=\"L\"))\n",
    "                test_label.append(one_hot_enc_arr)\n",
    "            else:\n",
    "                train_input.append(scipy.misc.imread(path + '/{}.png'.format(pic), mode=\"L\"))\n",
    "                train_label.append(one_hot_enc_arr)\n",
    "\n",
    "    train_input = np.expand_dims(train_input, -1)\n",
    "    train_label = np.array(train_label)\n",
    "    test_input = np.expand_dims(test_input, -1)\n",
    "    test_label = np.array(test_label)\n",
    "\n",
    "    return train_input, train_label, test_input, test_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_label, test_input, test_label = get_data(root_path, num_of_pics=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define size of placeholders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_hight = train_input.shape[1]\n",
    "image_width = train_input.shape[2]\n",
    "classes_numb = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, image_hight * image_width * 1], name='X_muliplied')\n",
    "\n",
    "X_shaped = tf.reshape(X, [-1, image_hight, image_width, 1], name='X_shaped_{}_{}'.format(image_hight, image_width))\n",
    "\n",
    "Y = tf.placeholder(tf.float32, [None, classes_numb], name=\"Y_labels\")\n",
    "\n",
    "neurons_in_first_dense = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for getting a convolutional layer\n",
    "# with particular parameters as:\n",
    "# strides for max pool is 2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_layer(input_data, num_chanels, num_filters, filter_shape, pool_shape, name):\n",
    "    conv_filter_shape = [filter_shape[0], filter_shape[1], num_chanels, num_filters]\n",
    "    weights = tf.Variable(tf.truncated_normal(conv_filter_shape, stddev=0.03), name=name + '_W')\n",
    "    biases = tf.Variable(tf.truncated_normal([num_filters], name=name + '_b'))\n",
    "    strides_conv = [1, 1, 1, 1]\n",
    "    out_layer = tf.nn.conv2d(input_data, weights, strides=strides_conv, padding='SAME')\n",
    "    out_layer += biases\n",
    "    out_layer = tf.nn.relu(out_layer)\n",
    "    ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
    "    strides = [1, 2, 2, 1]\n",
    "    out_layer = tf.nn.max_pool(out_layer, ksize=ksize, strides=strides, padding='SAME')\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for getting a fully connected layer layers\n",
    "# if last layer flag is false relu is apllied to the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_connected(shape=[None, None], prev_layer=None, is_last=False):\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=shape, stddev=.03, name='wd1'))\n",
    "    biases = tf.Variable(tf.truncated_normal(shape=[shape[1]], stddev=.01, name='bd1'))\n",
    "    dense_layer = tf.matmul(prev_layer, weights) + biases\n",
    "    if is_last:\n",
    "        return dense_layer\n",
    "\n",
    "    dense_layer = tf.nn.relu(dense_layer)\n",
    "    return dense_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construction the the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer_1 = get_conv_layer(X_shaped, 1, 32, [5, 5], [2, 2], name='layer_1')\n",
    "conv_layer_2 = get_conv_layer(conv_layer_1, 32, 64, [5, 5], [2, 2], name='layer_2')\n",
    "\n",
    "flattened_shape = functools.reduce(operator.mul, [i.value for i in conv_layer_2.shape[1:]], 1)\n",
    "flattened = tf.reshape(conv_layer_2, [-1, flattened_shape])\n",
    "\n",
    "dense_layer_1 = get_full_connected([flattened_shape, neurons_in_first_dense], prev_layer=flattened, is_last=False)\n",
    "dense_layer_2 = get_full_connected([neurons_in_first_dense, classes_numb], prev_layer=dense_layer_1, is_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Y_ for softmax output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ = tf.nn.softmax(dense_layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=dense_layer_2, labels=Y))\n",
    "optimiser = tf.train.AdamOptimizer().minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define methods for checking the accuracy and saving the models configs, \n",
    "# define glob_var initializator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "init = tf.global_variables_initializer()\n",
    "test_acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a session and plot the accuracy graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    test_acc_list = []\n",
    "    sess.run(init)\n",
    "    for i in range(50):\n",
    "        _, c = sess.run([optimiser, cross_entropy], feed_dict={X_shaped: train_input, Y: train_label})\n",
    "        test_acc = sess.run(accuracy, feed_dict={X_shaped: test_input, Y: test_label})\n",
    "        print(\"Epoch:\", (i + 1), ' cost: {}'.format(c), \" test accuracy: {:.3f}\".format(test_acc))\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "    #print(np.round(sess.run(Y_, feed_dict={X_shaped: train_input, Y: train_label}), 3))\n",
    "    #print(np.round(sess.run(Y, feed_dict={X_shaped: train_input, Y: train_label}), 3))\n",
    "\n",
    "    plt.plot(range(50), test_acc_list)\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
